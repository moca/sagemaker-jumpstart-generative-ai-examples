{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1463f153-71d2-480c-8565-a0adcdf2b21f",
   "metadata": {},
   "source": [
    "## In-context learning with FLAN-T5-XL "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690bda67-b335-4fe3-a72a-e360249dfc28",
   "metadata": {},
   "source": [
    "#### Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28f26242-65fd-471a-b1c8-a64bc686e32e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.model import Model\n",
    "from sagemaker import script_uris\n",
    "from sagemaker import image_uris \n",
    "from sagemaker import model_uris\n",
    "import sagemaker\n",
    "import logging\n",
    "import boto3\n",
    "import time\n",
    "import json\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c06035-ec39-4ebd-830f-49d4c164e600",
   "metadata": {},
   "source": [
    "#### Setup essentials "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dbe1bbc7-303f-495c-95a1-a1ae1ea39464",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logger = logging.getLogger('sagemaker')\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logger.addHandler(logging.StreamHandler())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c4be196-687a-492a-8a4e-97c5a4c5b2dd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using sagemaker==2.143.0\n",
      "Using boto3==1.26.103\n"
     ]
    }
   ],
   "source": [
    "logger.info(f'Using sagemaker=={sagemaker.__version__}')\n",
    "logger.info(f'Using boto3=={boto3.__version__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bb8dc308-3283-4dc1-b75e-9c4bb0309a89",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Role => arn:aws:iam::106877348565:role/service-role/AmazonSageMaker-ExecutionRole-20230120T102711\n"
     ]
    }
   ],
   "source": [
    "MODEL_ID = 'huggingface-text2text-flan-t5-xl'  # this is hard-coded\n",
    "MODEL_VERSION = '*'\n",
    "INSTANCE_TYPE = 'ml.p3.2xlarge'\n",
    "INSTANCE_COUNT = 1\n",
    "IMAGE_SCOPE = 'inference'\n",
    "MODEL_DATA_DOWNLOAD_TIMEOUT = 3600  # in seconds\n",
    "CONTAINER_STARTUP_HEALTH_CHECK_TIMEOUT = 3600\n",
    "EBS_VOLUME_SIZE = 256  # in GB\n",
    "CONTENT_TYPE = 'application/json'\n",
    "\n",
    "# set up roles and clients \n",
    "client = boto3.client('sagemaker-runtime')\n",
    "ROLE = get_execution_role()\n",
    "logger.info(f'Role => {ROLE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51be04ac-c509-47f8-bb83-537a689dc653",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Endpoint name: genai-paris-huggingface-text2text-flan-t5-xl-20230403220849\n"
     ]
    }
   ],
   "source": [
    "current_time = datetime.datetime.now().strftime('%Y%m%d%H%M%S')\n",
    "endpoint_name = f'genai-paris-{MODEL_ID}-{current_time}'\n",
    "logger.info(f'Endpoint name: {endpoint_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d67f166-291e-47ce-8cd2-48b06840e970",
   "metadata": {},
   "source": [
    "#### I. Deploy FLAN-T5-XL out-of-the-box instruction-tuned model as a SageMaker endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b17d8a09-28e1-4ad5-9383-c960ed79bca5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deploy image URI => 763104351884.dkr.ecr.eu-central-1.amazonaws.com/huggingface-pytorch-inference:1.10.2-transformers4.17.0-gpu-py38-cu113-ubuntu20.04\n",
      "Model URI => s3://jumpstart-cache-prod-eu-central-1/huggingface-infer/prepack/v1.0.3/infer-prepack-huggingface-text2text-flan-t5-xl.tar.gz\n"
     ]
    }
   ],
   "source": [
    "deploy_image_uri = image_uris.retrieve(\n",
    "    region=None, \n",
    "    framework=None, \n",
    "    image_scope=IMAGE_SCOPE, \n",
    "    model_id=MODEL_ID, \n",
    "    model_version=MODEL_VERSION, \n",
    "    instance_type=INSTANCE_TYPE\n",
    ")\n",
    "logger.info(f'Deploy image URI => {deploy_image_uri}')\n",
    "\n",
    "model_uri = model_uris.retrieve(\n",
    "    model_id=MODEL_ID, \n",
    "    model_version=MODEL_VERSION, \n",
    "    model_scope=IMAGE_SCOPE\n",
    ")\n",
    "\n",
    "logger.info(f'Model URI => {model_uri}')\n",
    "\n",
    "env = {\n",
    "    'SAGEMAKER_MODEL_SERVER_TIMEOUT': str(3600),\n",
    "    'MODEL_CACHE_ROOT': '/opt/ml/model', \n",
    "    'SAGEMAKER_ENV': '1',\n",
    "    'SAGEMAKER_SUBMIT_DIRECTORY': '/opt/ml/model/code/',\n",
    "    'SAGEMAKER_PROGRAM': 'inference.py',\n",
    "    'SAGEMAKER_MODEL_SERVER_WORKERS': '1',\n",
    "    'TS_DEFAULT_WORKERS_PER_MODEL': '1', \n",
    "}\n",
    "\n",
    "model = Model(\n",
    "    image_uri=deploy_image_uri, \n",
    "    model_data=model_uri, \n",
    "    role=ROLE, \n",
    "    predictor_cls=Predictor, \n",
    "    name=endpoint_name, \n",
    "    env=env\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "df2669fe-c5b3-4978-8eb4-61825f5e8276",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating model with name: genai-paris-huggingface-text2text-flan-t5-xl-20230403212649\n",
      "CreateModel request: {\n",
      "    \"ModelName\": \"genai-paris-huggingface-text2text-flan-t5-xl-20230403212649\",\n",
      "    \"ExecutionRoleArn\": \"arn:aws:iam::106877348565:role/service-role/AmazonSageMaker-ExecutionRole-20230120T102711\",\n",
      "    \"PrimaryContainer\": {\n",
      "        \"Image\": \"763104351884.dkr.ecr.eu-central-1.amazonaws.com/huggingface-pytorch-inference:1.10.2-transformers4.17.0-gpu-py38-cu113-ubuntu20.04\",\n",
      "        \"Environment\": {\n",
      "            \"SAGEMAKER_MODEL_SERVER_TIMEOUT\": \"3600\",\n",
      "            \"MODEL_CACHE_ROOT\": \"/opt/ml/model\",\n",
      "            \"SAGEMAKER_ENV\": \"1\",\n",
      "            \"SAGEMAKER_SUBMIT_DIRECTORY\": \"/opt/ml/model/code/\",\n",
      "            \"SAGEMAKER_PROGRAM\": \"inference.py\",\n",
      "            \"SAGEMAKER_MODEL_SERVER_WORKERS\": \"1\",\n",
      "            \"TS_DEFAULT_WORKERS_PER_MODEL\": \"1\"\n",
      "        },\n",
      "        \"ModelDataUrl\": \"s3://jumpstart-cache-prod-eu-central-1/huggingface-infer/prepack/v1.0.3/infer-prepack-huggingface-text2text-flan-t5-xl.tar.gz\"\n",
      "    },\n",
      "    \"Tags\": [\n",
      "        {\n",
      "            \"Key\": \"aws-jumpstart-inference-model-uri\",\n",
      "            \"Value\": \"s3://jumpstart-cache-prod-eu-central-1/huggingface-infer/prepack/v1.0.3/infer-prepack-huggingface-text2text-flan-t5-xl.tar.gz\"\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "Creating endpoint-config with name genai-paris-huggingface-text2text-flan-t5-xl-20230403212649\n",
      "Creating endpoint with name genai-paris-huggingface-text2text-flan-t5-xl-20230403212649\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------!CPU times: user 233 ms, sys: 18 ms, total: 251 ms\n",
      "Wall time: 6min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "_ = model.deploy(\n",
    "    initial_instance_count=INSTANCE_COUNT, \n",
    "    instance_type=INSTANCE_TYPE, \n",
    "    endpoint_name=endpoint_name, \n",
    "    volume_size=EBS_VOLUME_SIZE, \n",
    "    model_data_download_timeout=MODEL_DATA_DOWNLOAD_TIMEOUT, \n",
    "    container_startup_health_check_timeout=CONTAINER_STARTUP_HEALTH_CHECK_TIMEOUT\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739a2e2f-b55a-432b-be5f-6f101ed89ffe",
   "metadata": {},
   "source": [
    "#### II. Invoke the SageMaker endpoint to test the deployed model for natural language understanding (NLU) and natural language generation (NLG) tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "70c3a605-572c-49bd-8ffa-8851d86e255e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "context = \"\"\"\n",
    "Customer: Hi there, I'm having a problem with my iPhone.\n",
    "Agent: Hi! I'm sorry to hear that. What's happening?\n",
    "Customer: The phone is not charging properly, and the battery seems to be draining very quickly. I've tried different charging cables and power adapters, but the issue persists.\n",
    "Agent: Hmm, that's not good. Let's try some troubleshooting steps. Can you go to Settings, then Battery, and see if there are any apps that are using up a lot of battery life?\n",
    "Customer: Yes, there are some apps that are using up a lot of battery.\n",
    "Agent: Okay, try force quitting those apps by swiping up from the bottom of the screen and then swiping up on the app to close it.\n",
    "Customer: I did that, but the issue is still there.\n",
    "Agent: Alright, let's try resetting your iPhone's settings to their default values. This won't delete any of your data. Go to Settings, then General, then Reset, and then choose Reset All Settings.\n",
    "Customer: Okay, I did that. What's next?\n",
    "Agent: Now, let's try restarting your iPhone. Press and hold the power button until you see the \"slide to power off\" option. Slide to power off, wait a few seconds, and then turn your iPhone back on.\n",
    "Customer: Alright, I restarted it, but it's still not charging properly.\n",
    "Agent: I see. It looks like we need to run a diagnostic test on your iPhone. Please visit the nearest Apple Store or authorized service provider to get your iPhone checked out.\n",
    "Customer: Do I need to make an appointment?\n",
    "Agent: Yes, it's always best to make an appointment beforehand so you don't have to wait in line. You can make an appointment online or by calling the Apple Store or authorized service provider.\n",
    "Customer: Okay, will I have to pay for the repairs?\n",
    "Agent: That depends on whether your iPhone is covered under warranty or not. If it is, you won't have to pay anything. However, if it's not covered under warranty, you will have to pay for the repairs.\n",
    "Customer: How long will it take to get my iPhone back?\n",
    "Agent: It depends on the severity of the issue, but it usually takes 1-2 business days.\n",
    "Customer: Can I track the repair status online?\n",
    "Agent: Yes, you can track the repair status online or by calling the Apple Store or authorized service provider.\n",
    "Customer: Alright, thanks for your help.\n",
    "Agent: No problem, happy to help. Is there anything else I can assist you with?\n",
    "Customer: No, that's all for now.\n",
    "Agent: Alright, have a great day and good luck with your iPhone!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32db9f3e-1b84-483e-bb36-6b205ad4a51a",
   "metadata": {},
   "source": [
    "Generation configuration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5956c6d7-b8ee-4a89-ae62-df690444bc92",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MAX_LENGTH = 256\n",
    "NUM_RETURN_SEQUENCES = 1\n",
    "TOP_K = 0\n",
    "TOP_P = 0.7\n",
    "DO_SAMPLE = True \n",
    "\n",
    "\n",
    "sm = boto3.client('sagemaker')\n",
    "r = client.list_endpoints(StatusEquals='InService')\n",
    "endpoint_name = [ ep['EndpointName'] for ep in r['Endpoints'] if  'flan-t5' in ep['EndpointName'] ][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a896590-26f1-4a75-bb6a-7636b409c5b7",
   "metadata": {},
   "source": [
    "#### A. Text Summarization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bc0b66d8-cc52-416a-8a43-c00ce5947577",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Response: Customer's iPhone is not charging properly, and the battery is draining very quickly. Agent: Can you go to Settings, then Battery, and see if there are any apps that are using up a lot of battery? Force quit apps that are using up a lot of battery. Reset your iPhone's settings to their default values. Restart your iPhone. Contact Apple Store or authorized service provider. Make an appointment to get your iPhone checked out.\n"
     ]
    }
   ],
   "source": [
    "query = 'write a summary'\n",
    "prompt = f'{context}\\n{query}'\n",
    "\n",
    "payload = {\n",
    "    'text_inputs': prompt, \n",
    "    'max_length': MAX_LENGTH, \n",
    "    'num_return_sequences': NUM_RETURN_SEQUENCES,\n",
    "    'top_k': TOP_K,\n",
    "    'top_p': TOP_P,\n",
    "    'do_sample': DO_SAMPLE\n",
    "}\n",
    "\n",
    "payload = json.dumps(payload).encode('utf-8')\n",
    "\n",
    "response = client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name, \n",
    "    ContentType=CONTENT_TYPE, \n",
    "    Body=payload\n",
    ")\n",
    "\n",
    "model_predictions = json.loads(response['Body'].read())\n",
    "generated_text = model_predictions['generated_texts'][0]\n",
    "logger.info(f'Response: {generated_text}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f297221e-f7fa-4b4e-9e93-634ba647dbce",
   "metadata": {},
   "source": [
    "#### B. Abstractive Question Answering "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af902e9c-7ba3-4d0e-890f-3235000208cc",
   "metadata": {},
   "source": [
    "##### Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "76d4ec73-27c4-497a-930c-d83bd769f1e0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Response: Force quit apps using a lot of battery. Reset your iPhone's settings to default. Restart your iPhone. Get your iPhone checked out.\n"
     ]
    }
   ],
   "source": [
    "query = 'What troubleshooting steps were suggested to the customer to fix their iPhone charging issue?'\n",
    "\n",
    "prompt = f'{context}\\n{query}'\n",
    "\n",
    "payload = {\n",
    "    'text_inputs': prompt, \n",
    "    'max_length': MAX_LENGTH, \n",
    "    'num_return_sequences': NUM_RETURN_SEQUENCES,\n",
    "    'top_k': TOP_K,\n",
    "    'top_p': TOP_P,\n",
    "    'do_sample': DO_SAMPLE\n",
    "}\n",
    "\n",
    "payload = json.dumps(payload).encode('utf-8')\n",
    "\n",
    "response = client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name, \n",
    "    ContentType=CONTENT_TYPE, \n",
    "    Body=payload\n",
    ")\n",
    "\n",
    "model_predictions = json.loads(response['Body'].read())\n",
    "generated_text = model_predictions['generated_texts'][0]\n",
    "logger.info(f'Response: {generated_text}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d7781a-2184-4583-91b0-5d925ada857b",
   "metadata": {},
   "source": [
    "Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "722e1c72-5450-4efd-858e-937bc499c73c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Response: No, the problem persists.\n"
     ]
    }
   ],
   "source": [
    "query = 'Was resetting the iPhone to its default settings able to solve the charging issue and battery drain problem?'\n",
    "\n",
    "prompt = f'{context}\\n{query}'\n",
    "\n",
    "payload = {\n",
    "    'text_inputs': prompt, \n",
    "    'max_length': MAX_LENGTH, \n",
    "    'num_return_sequences': NUM_RETURN_SEQUENCES,\n",
    "    'top_k': TOP_K,\n",
    "    'top_p': TOP_P,\n",
    "    'do_sample': DO_SAMPLE\n",
    "}\n",
    "\n",
    "payload = json.dumps(payload).encode('utf-8')\n",
    "\n",
    "response = client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name, \n",
    "    ContentType=CONTENT_TYPE, \n",
    "    Body=payload\n",
    ")\n",
    "\n",
    "model_predictions = json.loads(response['Body'].read())\n",
    "generated_text = model_predictions['generated_texts'][0]\n",
    "logger.info(f'Response: {generated_text}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b668ab9a-8c0e-4668-b2ea-c53471a09569",
   "metadata": {},
   "source": [
    "Q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "59d2cfbc-c5c5-4766-9a96-3fe66d2b4288",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Response: Customer should make an appointment online or by calling the Apple Store or authorized service provider.\n"
     ]
    }
   ],
   "source": [
    "query = 'What steps can the customer take to make an appointment at the nearest Apple Store or authorized service provider for iPhone repair?'\n",
    "\n",
    "prompt = f'{context}\\n{query}'\n",
    "\n",
    "payload = {\n",
    "    'text_inputs': prompt, \n",
    "    'max_length': MAX_LENGTH, \n",
    "    'num_return_sequences': NUM_RETURN_SEQUENCES,\n",
    "    'top_k': TOP_K,\n",
    "    'top_p': TOP_P,\n",
    "    'do_sample': DO_SAMPLE\n",
    "}\n",
    "\n",
    "payload = json.dumps(payload).encode('utf-8')\n",
    "\n",
    "response = client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name, \n",
    "    ContentType=CONTENT_TYPE, \n",
    "    Body=payload\n",
    ")\n",
    "\n",
    "model_predictions = json.loads(response['Body'].read())\n",
    "generated_text = model_predictions['generated_texts'][0]\n",
    "logger.info(f'Response: {generated_text}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b85ca1-92f5-4d5a-b5d5-82f8655d49c0",
   "metadata": {},
   "source": [
    "#### C. Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "be1fd8d0-f903-4d9d-bfee-b3d070615065",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Response: positive\n"
     ]
    }
   ],
   "source": [
    "query = 'What is the overall sentiment and sentiment score of the conversation between the customer and the agent'\n",
    "prompt = f'{context}\\n{query}'\n",
    "\n",
    "payload = {\n",
    "    'text_inputs': prompt, \n",
    "    'max_length': MAX_LENGTH, \n",
    "    'num_return_sequences': NUM_RETURN_SEQUENCES,\n",
    "    'top_k': TOP_K,\n",
    "    'top_p': TOP_P,\n",
    "    'do_sample': DO_SAMPLE\n",
    "}\n",
    "\n",
    "payload = json.dumps(payload).encode('utf-8')\n",
    "\n",
    "response = client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name, \n",
    "    ContentType=CONTENT_TYPE, \n",
    "    Body=payload\n",
    ")\n",
    "\n",
    "model_predictions = json.loads(response['Body'].read())\n",
    "sentiment = model_predictions['generated_texts'][0]\n",
    "logger.info(f'Response: {sentiment}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8425737b-6c30-4f98-ac08-1f3e9a0aca33",
   "metadata": {},
   "source": [
    "#### D. Sentiment Phrase Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e51a90f0-2251-478c-b178-c7d9ba32dc66",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Response: agent\n"
     ]
    }
   ],
   "source": [
    "query = f'identify any specific words, phrases, or context that influenced the {sentiment} sentiment'\n",
    "\n",
    "prompt = f'{context}\\n{query}'\n",
    "\n",
    "payload = {'text_inputs': prompt, \n",
    "           'max_length': MAX_LENGTH, \n",
    "           'num_return_sequences': NUM_RETURN_SEQUENCES,\n",
    "           'top_k': TOP_K,\n",
    "           'top_p': TOP_P,\n",
    "           'do_sample': DO_SAMPLE}\n",
    "\n",
    "payload = json.dumps(payload).encode('utf-8')\n",
    "\n",
    "response = client.invoke_endpoint(EndpointName=endpoint_name, \n",
    "                                  ContentType=CONTENT_TYPE, \n",
    "                                  Body=payload)\n",
    "\n",
    "model_predictions = json.loads(response['Body'].read())\n",
    "generated_text = model_predictions['generated_texts'][0]\n",
    "logger.info(f'Response: {generated_text}')"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-central-1:936697816551:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
